{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining & Analytics\n",
    "## March 15 , Lab 6 (A): Skip Gram models\n",
    "\n",
    "Available software:\n",
    " - Python's Gensim module: https://radimrehurek.com/gensim/ (install using pip)\n",
    " - Sklearn’s  TSNE module in case you use TSNE to reduce dimension (optional)\n",
    " - Python’s Matplotlib (optional)\n",
    "\n",
    "_Note: The most important hyper parameters of skip-gram/CBOW are vector size and windows size_\n",
    "\n",
    "This assignment  will be broadly  split into **2 parts**.\n",
    "\n",
    "#### Part I\n",
    "##### Preparation:\n",
    "Download and extract the Google’s pretrained Word2Vec model (Google has  trained a large corpus of text containing billions of words,). To kick things off we will use this pre trained model to explore Word2Vec. \n",
    "(Download Link: https://docs.google.com/a/berkeley.edu/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download)\n",
    "Now load this pretrained model in Gensim and you should be good to get started with this assignment. \n",
    "\n",
    "\n",
    "\n",
    "Q1: Find the **cosine similarities** between the following word pairs/tuples:\n",
    "- (France, England)\n",
    "- (smaller, bigger)\n",
    "- (England, London)\n",
    "- (France, Rocket)\n",
    "- (big, bigger)\n",
    "\n",
    "Q2 : Write an expression to extract the vector representations   of the words  (France,  England, smaller, bigger, rocket, big). \n",
    "\n",
    "Q3: Repeat the exercise from Q1 by finding the **euclidean distances** between the word pairs.\n",
    "\n",
    "Q4: What is the relationship between the magnitude of individual vectors, the vectors themselves and the cosine distance for any pair of words. Use any tuple in Q1 as an example to support your answer. \n",
    "\n",
    "Q5: Time to dabble with the power of Word2Vec. Find the 2 closest words  for the following conditions:  \n",
    "- (King - Queen)\n",
    "- (bigger - big + small)\n",
    "- (man + programmer - woman)\n",
    "- (school + shooting - guns)\n",
    "- (Texas + Milwaukee – Wisconsin)\n",
    "\n",
    "Q6: Using the vectors for the words in the Google News dataset, explore the semantic representation of these words through K-means clustering and explain your findings.\n",
    "\n",
    "Q7: What loss function does the skipgram model use and briefly describe what this function is minimizing .\n",
    "\n",
    "\n",
    "#### Part II:\n",
    "\n",
    "In part 1 we used the Word2Vec model on a pre trained corpus. In this part (in the next lab) you are going to train a Word2Vec model on your own dataset/corpus(text). Choose a text corpus (A good place to start will be the nltk corpus, the gutenburg project or the brown movie reviews) and tokenize the text (We will go through this in detail in the next Lab.) \n",
    "\n",
    "You can also choose the the dataset provided in the assignment page for this lab.\n",
    "\n",
    "Q7. Based on your knowledge or understand of the text corpus you have chosen, form 3 hypotheses of analogies or relationships you expect will hold and give a reason why.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
