{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining & Analytics\n",
    "## March 15 , Lab 6 (A): Skip Gram models\n",
    "\n",
    "Available software:\n",
    " - Python's Gensim module: https://radimrehurek.com/gensim/ (install using pip)\n",
    " - Sklearn’s  TSNE module in case you use TSNE to reduce dimension (optional)\n",
    " - Python’s Matplotlib (optional)\n",
    "\n",
    "_Note: The most important hyper parameters of skip-gram/CBOW are vector size and windows size_\n",
    "\n",
    "This assignment  will be broadly  split into **2 parts**.\n",
    "\n",
    "#### Part I\n",
    "##### Preparation:\n",
    "Download and extract the Google’s pretrained Word2Vec model (Google has  trained a large corpus of text containing billions of words,). To kick things off we will use this pre trained model to explore Word2Vec. \n",
    "(Download Link: https://docs.google.com/a/berkeley.edu/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download)\n",
    "Now load this pretrained model in Gensim and you should be good to get started with this assignment. \n",
    "\n",
    "\n",
    "\n",
    "Q1: Find the **cosine similarities** between the following word pairs/tuples:\n",
    "- (France, England)\n",
    "- (smaller, bigger)\n",
    "- (England, London)\n",
    "- (France, Rocket)\n",
    "- (big, bigger)\n",
    "\n",
    "Q2 : Write an expression to extract the vector representations   of the words  (France,  England, smaller, bigger, rocket, big). \n",
    "\n",
    "Q3: Repeat the exercise from Q1 by finding the **euclidean distances** between the word pairs.\n",
    "\n",
    "Q4: What is the relationship between the magnitude of individual vectors, the vectors themselves and the cosine distance for any pair of words. Use any tuple in Q1 as an example to support your answer. \n",
    "\n",
    "Q5: Time to dabble with the power of Word2Vec. Find the 2 closest words  for the following conditions:  \n",
    "- (King - Queen)\n",
    "- (bigger - big + small)\n",
    "- (man + programmer - woman)\n",
    "- (school + shooting - guns)\n",
    "- (Texas + Milwaukee – Wisconsin)\n",
    "\n",
    "Q6: Using the vectors for the following words, explore the semantic representation of these words through K-means clustering and explain your findings.\n",
    "\n",
    "Q7: What loss function does the skipgram model use and briefly describe what this function is minimizing .\n",
    "\n",
    "\n",
    "#### Part II:\n",
    "\n",
    "In part 1 we used the Word2Vec model on a pre trained corpus. In this part (in the next lab) you are going to train a Word2Vec model on your own dataset/corpus(text). Choose a text corpus (A good place to start will be the nltk corpus, the gutenburg project or the brown movie reviews) and tokenize the text (We will go through this in detail in the next Lab.) \n",
    "\n",
    "You can also choose the the dataset provided here.\n",
    "\n",
    "Q7. Based on your knowledge or understand of the text corpus you have chosen, form 3 hypotheses of analogies or relationships you expect will hold and give a reason why.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors,Word2Vec\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6064219583833188\n",
      "0.7302272087331374\n",
      "0.43992858122481976\n",
      "0.07114174367741376\n",
      "0.6842385598892738\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.similarity('France','Spain'))\n",
    "print(word_vectors.similarity('smaller','bigger'))\n",
    "print(word_vectors.similarity('England','London'))\n",
    "print(word_vectors.similarity('France','Rocket'))\n",
    "print(word_vectors.similarity('big','bigger'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.85839844e-02,  7.86132812e-02,  3.24218750e-01,  3.49121094e-02,\n",
       "        7.71484375e-02,  3.54003906e-02, -1.25976562e-01, -3.86718750e-01,\n",
       "       -1.31835938e-01,  2.91748047e-02, -1.44531250e-01, -1.42578125e-01,\n",
       "        1.79687500e-01, -2.75390625e-01, -1.65039062e-01,  9.32617188e-02,\n",
       "        1.17187500e-01,  1.82617188e-01,  6.10351562e-02,  1.14257812e-01,\n",
       "        1.82617188e-01, -1.16699219e-01, -3.24707031e-02, -7.56835938e-02,\n",
       "        9.64355469e-03,  8.59375000e-02, -2.85156250e-01, -2.55859375e-01,\n",
       "        3.01513672e-02,  2.16796875e-01, -1.00097656e-01,  2.85644531e-02,\n",
       "       -2.81250000e-01, -8.39843750e-02, -2.02636719e-02, -1.96289062e-01,\n",
       "       -4.78515625e-02,  7.12890625e-02, -1.42578125e-01, -1.13525391e-02,\n",
       "        1.16210938e-01,  7.22656250e-02,  1.47460938e-01,  1.50390625e-01,\n",
       "        1.40625000e-01,  2.47070312e-01, -1.69921875e-01,  7.76367188e-02,\n",
       "       -5.44433594e-02,  1.66992188e-01, -1.45507812e-01,  2.12402344e-02,\n",
       "       -7.51953125e-02,  4.58984375e-02, -2.55859375e-01,  1.49414062e-01,\n",
       "       -5.62500000e-01, -1.34765625e-01, -1.87500000e-01, -3.26538086e-03,\n",
       "        8.44726562e-02,  1.33789062e-01,  2.99072266e-02, -2.92968750e-01,\n",
       "       -1.56250000e-01,  2.50244141e-03, -1.10473633e-02, -7.08007812e-02,\n",
       "        1.90429688e-02,  1.56250000e-01, -4.95605469e-02,  2.08007812e-01,\n",
       "       -3.66210938e-02,  2.07031250e-01,  1.27929688e-01,  2.91748047e-02,\n",
       "       -4.88281250e-03, -6.31713867e-03,  6.49414062e-02,  1.66015625e-01,\n",
       "       -1.08032227e-02, -1.83593750e-01,  1.49414062e-01, -1.71875000e-01,\n",
       "        1.85546875e-01, -9.86328125e-02, -2.10937500e-01, -2.06298828e-02,\n",
       "        1.02050781e-01,  1.41601562e-01, -1.21093750e-01,  5.93261719e-02,\n",
       "       -2.89062500e-01,  7.47070312e-02,  3.11279297e-02, -2.21679688e-01,\n",
       "        1.94335938e-01, -4.62890625e-01, -6.78710938e-02, -1.91650391e-02,\n",
       "       -2.39257812e-01, -1.09863281e-01, -1.45507812e-01,  1.81640625e-01,\n",
       "        1.27929688e-01, -1.41601562e-01,  8.39843750e-02, -6.56127930e-03,\n",
       "        8.83789062e-02,  2.75878906e-02, -5.61523438e-03,  8.88671875e-02,\n",
       "        2.25585938e-01, -3.49121094e-02,  1.72851562e-01, -2.64892578e-02,\n",
       "        1.61743164e-03,  2.17773438e-01,  3.51562500e-01,  1.61132812e-01,\n",
       "       -8.20312500e-02, -1.36718750e-01,  2.20947266e-02, -2.96630859e-02,\n",
       "        2.77343750e-01, -1.67968750e-01, -3.08837891e-02, -1.59179688e-01,\n",
       "       -4.00390625e-01,  8.93554688e-02,  2.99072266e-02,  1.82617188e-01,\n",
       "       -3.06640625e-01, -1.74804688e-01, -9.09423828e-03, -4.51660156e-02,\n",
       "        4.34570312e-02, -1.25732422e-02,  1.57226562e-01, -7.66601562e-02,\n",
       "        2.36328125e-01, -1.10839844e-01,  2.11914062e-01, -5.29785156e-02,\n",
       "        1.22558594e-01,  9.39941406e-03,  1.25976562e-01,  4.85839844e-02,\n",
       "       -4.76074219e-02, -2.09960938e-01, -1.42578125e-01,  1.34277344e-02,\n",
       "       -3.35693359e-03,  8.39843750e-02,  1.03515625e-01, -3.14453125e-01,\n",
       "        8.48388672e-03,  1.78710938e-01,  9.47265625e-02,  1.61132812e-01,\n",
       "       -4.76074219e-02,  1.91406250e-01,  2.09960938e-01, -7.17773438e-02,\n",
       "        2.94921875e-01, -1.96289062e-01, -1.78710938e-01,  1.10351562e-01,\n",
       "       -1.77001953e-02,  9.66796875e-02, -1.20605469e-01,  1.40625000e-01,\n",
       "        6.34765625e-02, -1.93359375e-01, -7.27539062e-02, -6.64062500e-02,\n",
       "        4.46777344e-02, -1.25000000e-01,  1.26953125e-01, -1.02539062e-01,\n",
       "        1.30859375e-01, -2.37304688e-01, -3.10546875e-01, -2.39257812e-01,\n",
       "        4.23828125e-01, -3.55529785e-03,  1.85546875e-01,  2.38281250e-01,\n",
       "       -1.48437500e-01,  1.91406250e-01,  1.20117188e-01,  5.73730469e-02,\n",
       "        1.11816406e-01,  9.52148438e-03, -1.42822266e-02,  3.33984375e-01,\n",
       "        1.17675781e-01, -8.44726562e-02, -1.66992188e-01, -2.12890625e-01,\n",
       "        1.13281250e-01,  3.71093750e-02, -6.00585938e-02, -1.61132812e-01,\n",
       "        1.28906250e-01,  1.63085938e-01, -1.76757812e-01, -2.05078125e-01,\n",
       "        7.95898438e-02, -7.95898438e-02,  5.39550781e-02, -1.28906250e-01,\n",
       "       -8.64257812e-02,  3.75000000e-01, -1.26953125e-01, -1.11328125e-01,\n",
       "        1.81884766e-02, -1.55273438e-01,  1.50390625e-01, -4.27246094e-02,\n",
       "       -1.30859375e-01, -1.69921875e-01, -6.16455078e-03, -3.06396484e-02,\n",
       "       -1.56250000e-01,  7.37304688e-02,  2.59765625e-01,  8.49609375e-02,\n",
       "        3.71093750e-01,  9.22851562e-02, -9.81445312e-02, -3.39355469e-02,\n",
       "        2.01171875e-01,  7.12890625e-02, -5.49316406e-02,  3.61328125e-02,\n",
       "       -9.32617188e-02,  7.56835938e-02, -2.29492188e-01, -8.15429688e-02,\n",
       "        8.88671875e-02, -6.03027344e-02,  1.64062500e-01,  2.35351562e-01,\n",
       "        1.41601562e-01,  7.03125000e-02, -3.90625000e-01, -1.15722656e-01,\n",
       "       -2.25585938e-01, -1.12792969e-01,  8.59375000e-02,  7.81250000e-02,\n",
       "       -2.20947266e-02,  1.56250000e-01,  4.76074219e-02,  1.19140625e-01,\n",
       "        3.61328125e-02, -1.98242188e-01,  7.47070312e-02,  1.72851562e-01,\n",
       "        6.98242188e-02,  2.75390625e-01, -3.36914062e-02,  2.49023438e-01,\n",
       "        2.17773438e-01, -6.03027344e-02, -1.47460938e-01, -2.92968750e-01,\n",
       "       -1.18164062e-01,  1.43554688e-01, -1.95312500e-01,  7.56835938e-02,\n",
       "       -3.56445312e-02, -1.07910156e-01,  7.51953125e-02,  9.81445312e-02,\n",
       "       -6.59179688e-02,  2.65625000e-01, -1.48437500e-01,  4.93164062e-02,\n",
       "       -2.37304688e-01,  2.50244141e-02,  8.69750977e-04, -3.47900391e-03,\n",
       "       -8.25195312e-02,  2.43164062e-01, -2.49023438e-02,  1.49414062e-01,\n",
       "       -6.64062500e-02,  4.95605469e-02,  1.38671875e-01,  3.16406250e-01,\n",
       "       -6.58035278e-05,  1.62109375e-01, -1.54296875e-01, -7.71484375e-02,\n",
       "       -1.19628906e-01, -2.22167969e-02,  1.60156250e-01, -7.17773438e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example with France\n",
    "word_vectors['France']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4046993255615234\n",
      "1.8618743419647217\n",
      "2.875283718109131\n",
      "3.892071008682251\n",
      "1.9586496353149414\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "print(distance.euclidean(word_vectors['France'],word_vectors['Spain']))\n",
    "print(distance.euclidean(word_vectors['smaller'],word_vectors['bigger']))\n",
    "print(distance.euclidean(word_vectors['England'],word_vectors['London']))\n",
    "print(distance.euclidean(word_vectors['France'],word_vectors['Rocket']))\n",
    "print(distance.euclidean(word_vectors['big'],word_vectors['bigger']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.b=norm(a)*norm(b)*cos(a,b) (dot product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.782578823519442\n",
      "5.782578846356046\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.linalg.norm(word_vectors['France'])*np.linalg.norm(word_vectors['Spain'])*word_vectors.similarity('France','Spain')\n",
    "b = np.linalg.norm(word_vectors['France'])**2+np.linalg.norm(word_vectors['Spain'])**2\n",
    "print(b-2*a)\n",
    "print(distance.euclidean(word_vectors['France'],word_vectors['Spain'])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bon_Jovi_Canillas', 0.36876532435417175), ('Lomax', 0.36203980445861816)]\n",
      "[('larger', 0.7402472496032715), ('smaller', 0.7329993844032288)]\n",
      "[('programer', 0.5371962785720825), ('programmers', 0.5310998558998108)]\n",
      "[('elementary', 0.5435296297073364), ('eighth_grade', 0.47330963611602783)]\n",
      "[('Houston', 0.7767742872238159), ('Fort_Worth', 0.7270511388778687)]\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.most_similar(positive=['King'], negative=['Queen'])[:2])\n",
    "print(word_vectors.most_similar(positive=['bigger','small'], negative=['big'])[:2])\n",
    "print(word_vectors.most_similar(positive=['man','programmer'], negative=['woman'])[:2])\n",
    "print(word_vectors.most_similar(positive=['school','shooting'], negative=['guns'])[:2])\n",
    "print(word_vectors.most_similar(positive=['Texas','Milwaukee'], negative=['Wisconsin'])[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The used loss function is NCE. Since we are outputing a very huge softmax vector. It is quicker to train a model by sampling over small randomly-chosen subset of contrastive classes (called candidates) for each batch of training examples. Basically it is the same as a softmax but on smaller output vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_vec = word_vectors.vectors[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X_embedded = TSNE(n_components=2).fit_transform(subset_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEWCAYAAACT7WsrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XeYVOXZx/HvLUURiCKsBJEigoolNlA2xoJiL6ivokYT1Bii0VhjIxoNib0nlgSjEY0NsWBJjG3FirIoKkooIlXKqoANC3q/fzxnssOyMzs7zMyZ8vtc11wz58zMOfcchrn36ebuiIiINGaNuAMQEZHipSQhIiIpKUmIiEhKShIiIpKSkoSIiKSkJCEiIikpSRQJM7vYzP5ZgPP0NDM3s5bR9vNmdkK+z1sIufwsZnaHmf0pi/e5mfXORQwpjr+zmU3N1/EbOV9eP0+2zGy4mf09T8eeZWaDUjyX1feilClJFIiZfZ50+97MlidtH53jc91hZt80OOdbuTxHtpKS1JsN9neKYp6V4XEKklSLjbu/6O6b5uPYxfoHg5ntZmbzkve5+6XuXnSxliMliQJx93aJGzAHODBp3915OOWVyed0963zcI7VsbaZbZm0/VPgg7iCEZHGKUkUl9ZmdqeZfWZm75pZv8QTZraBmT1oZnVm9oGZnZrD825sZq+b2admNtbM1ks670FRLEujvzT7RvuPM7PHkl433cweSNqea2bbpDnnXcDQpO2fA3cmvyDVZzazfYDhwBGNlJJ6mNnL0TV8ysw6NfVZoue2NbM3ovfdD6yVKnAz621m48xsmZl9FL0+2aDoeiw1s5vMzKL3rWFmF5jZbDNbHP1brxM9N8rMzooed41KWydH2xub2SfR+1f6qzqqGvmtmb0dxXO/ma2V9Pw5ZrbAzD40sxNSVR+Z2SXAzsCN0TW9sanPE73veDObYmZLzOw/ZtYjzXVLd/1nmdn5ZvZedKx/mNlaZtYW+DewQVKpeIPkkqTVl06Pi753S8zsRDPrH12XpcmfJ7qez5nZx9G/391mtm6quNN8nvZmVmNmf06+JmXH3XUr8A2YBQxqsO9i4CtgP6AFcBkwPnpuDWAi8HugNdALmAnsneL4dwB/SvFcT8CBltH288B8YEugLfAg8M/ouU2AL4A9gVbAOcCMpBiWRrFtAMwG5kXv6wUsAdZIc/6ewNzos24O/BcYBMzK5DNH1+ufDY79PPB+FHebaPvyDD5L6yj+M6LnDgO+TXMN7wV+F8W4FvCTpOcceBxYF+gO1AH7RM8dH52zF9AOeAi4K+m5x6LHP40+x/1Jz42NHu+WuM5J36XXo3+D9YApwInRc/sAC4EtgLWBf0bx9U7xuZ4HTmiwL93nGRx9nr5AS+AC4JUUx055/ZM+x2SgW/Q5Xk5c/4afueG/P/Xfqb9G/x57Ef4vPQKsD3QFFgO7Rq/vHcWxJlAFvABcn+7/Z8P/W0DH6Lo3+h0pp5tKEsXlJXf/l7t/R/hLO1FF1B+ocvcR7v6Nu88EbgWOTHOs30Z/QSVuo9K89i53n+zuXwAXAkPMrAVwBPCEuz/t7t8CVxN+fH8cxfAZsA2wC/Af4EMz2wzYFXjR3b9Pc855wFRCYvh59HmTZfOZAf7h7tPcfTkwOoqPdJ8FGED44bre3b919zHAhDTn+BboAWzg7l+5+0sNnr/c3Ze6+xygJimGo4Fr3X2mu38OnA8caaETwTjgJ2a2BuF6XgnsFL1v1+j5VP7s7h+6+yfAY0nnGxJdj3fd/UvCD2s2Un2eE4HL3H2Ku68ALgW2SVGaSHf9E25097nR57gEOKqZcf4x+vd4ipCQ7nX3xe4+H3gR2BbA3WdEcXzt7nXAtYRrnKkNCP8eD7j7Bc2MseQoSRSXhUmPvwTWin5AehCK2//70SdUt3ROc6yr3X3dpNvQNK+dm/R4NuEHsxP1JQQAoh/9uYS/zCD8R9mN8KM2jvCX6K40/aOWcCdwLOHHoGGSyOYzw6rXsF30ON1n2QCY79GfipHZpHYOYMDrUfXJ8dnEED1uCXR29/cJP2zbEKp9Hick3U1p+nqmO1/yv23y4+ZIdfwewA1J/z6fEK5LV1bV1HepYXyzo/c0x6Kkx8sb2W4HYGadzew+M5tvZp8SSlidyNz+hAT312bGV5KUJErDXOCDBj/67d19vxwdv1vS4+6Ev5Q/Aj4k/BAAENW7diNUT0F9ktg5ejyO5iWJBwn/4WZGf6Uma+ozN3f64nSfZQHQtUG9cvdUB3L3he7+S3ffAPgVcHNj9fxNxRCdYwX1P2bjCFVdraO/fscR2m06AJMyOH5DC4ANk7a7pXphpLnXdC7wqwb/Rm3c/ZVGXtvUd6lhfN2j92QTV1MujY65lbv/ADiGkNwydSvwJPCvqM2krClJlIbXgc/M7Fwza2NmLcxsSzPrn6PjH2Nmm5vZ2sAIYExU5TUa2N/M9jCzVsBZwNdA4kdgHDAQaOPu8whF+n0I9bVvNjxJQ1H11u5AY10Zm/rMi4CeUfVMJtJ9llcJP9anmlkrMzsU2CHVgczscDNL/PguIfzgpKtaS7gXOMPMNjKzdoQfq/ujqhoI1/MUQh05hJLZKYRqyO8y/JzJRgPHmVnf6N/2wiZev4jQXpKpvwLnm9kWAGa2jpkdniaWdN8lgJPNbEMLHSd+ByQ6BCwCOlrUyJ8D7YHPgWVm1hU4O4tjnEKoLn3MzNrkKK6ipCRRAqIfiAMIVREfEP7K/zuQ7j/NObbyOImP0rz2LkKD3EJCw9+p0XmnEv7K+kt0zgMJXXe/iZ6fRvjP9mK0/SmhcfnlTH/U3L02qmpp7mdO9KT62MzeyOA8KT9L9HkOJVR9fUKoP38ozeH6A6+Z2efAo8BpUZtJU24nXOsXos/0FfCbpOfHEX7AEkniJUKD8wtkwd3/DfyZ0I4wAxgfPfV1irfcABwW9Q76cwbHfxi4ArgvqraZDOyb4rVpv0uRe4CnCN+h9wkNxLj7fwkJdmZUtdXcaqiG/gBsBywDniD9v3WjoqrJYYS2tbGW1KOs3NjK1bAiUq6iLqeTgTWTSi9FwcIgyhPc/Zm4Y5GVqSQhUsbM7BAzW9PMOhD+6n+s2BKEFDclCZHy9ivCGIH3ge+Ak+INR0qNqptERCQllSRERCSllnEHsDo6derkPXv2jDsMEZGSMnHixI/cvSqT15Z0kujZsye1tbVxhyEiUlLMLN2MAitRdZOIiKSkJCEiIikpSYiISEpKEiIikpKShIiIpFRRSeLKK6GmZuV9NTVhv4iIrKqikkT//jBkSH2iqKkJ2/1zNeG2iEiZKelxEs01cCCMHg2HHgqbbw7TpoXtgQPjjkxEpDhVVEkCQkIYMABeeQV++lMlCBGRdPKWJMzsdjNbbGaTk/atZ2ZPm9n06L5DtN/M7M9mNsPM3jaz7fIVV00NjI+WXvnHP1ZtoxARkXr5LEncQVjKMtl5wLPu3gd4NtqGsJpVn+g2DLglHwEl2iBGj4Y2bWDQoJXbKEREZGV5SxLu/gJhKchkg4FR0eNRwMFJ++/0YDywrpl1yXVMEyaEBLHnnqGxet68sD1hQq7PJCJSHgrdcN3Z3RdEjxcCnaPHXYG5Sa+bF+1bQANmNoxQ2qB79+7NOvk559Q/rq6Ga64J7RNqlxARaVxsDdfRQuLNXvHI3Ue6ez9371dVldFMt42qroYVK+CNN7I+hIhI2St0kliUqEaK7hdH++cD3ZJet2G0L2+qq8P9q6/m8ywiIqWt0EniUWBo9HgoMDZp/8+jXk4DgGVJ1VJ5sf760KuXkoSISDp5a5Mws3uB3YBOZjYPuAi4HBhtZr8AZgNDopf/C9gPmAF8CRyXr7iSVVfDc8+BO5gV4owiIqUlb0nC3Y9K8dQejbzWgZPzFUsq1dVw990wZw706FHos4uIFL+KG3GdTO0SIiLpVXSS+NGPYO21lSRERFKp6CTRsmUYVKckISLSuIpOEhCqnN58E5YvjzsSEZHioyQRDaqbODHuSEREik/FJ4kBA8K9qpxERFZV8Uli/fVh442VJEREGlPxSQJCaeLVV8OgOhERqackQWiXWLgQZs+OOxIRkeKiJIEG1YmIpKIkgQbViYikoiSBBtWJiKSiJBGproZJkzSoTkQkmZJEJDGorrY27khERIqHkkREg+pERFYVS5Iws9PMbLKZvWtmp0f71jOzp81senTfoZAxaVCdiMiqCp4kzGxL4JfADsDWwAFm1hs4D3jW3fsAz0bbBVVdrUF1IiLJ4ihJ9AVec/cv3X0FMA44FBgMjIpeMwo4uNCBVVfDokUwa1ahzywiUpziSBKTgZ3NrKOZrU1Y27ob0NndF0SvWQh0buzNZjbMzGrNrLauri6ngWlQnYjIygqeJNx9CnAF8BTwJDAJ+K7BaxxotNLH3Ue6ez9371dVVZXT2LbaCtq2VZIQEUmIpeHa3W9z9+3dfRdgCTANWGRmXQCi+8WFjkuD6kREVhZX76b1o/vuhPaIe4BHgaHRS4YCY+OIrboa3noLvvwyjrOLiBSXuMZJPGhm7wGPASe7+1LgcmBPM5sODIq2C06D6kRE6rWM46TuvnMj+z4G9oghnJUkD6rbZZd4YxERiZtGXDdQVQW9e8P48XFHIiISPyWJRmhQnYhIoCTRCA2qExEJlCQaoUF1IiKBkkQjttxSg+pEREBJolEtW8IOOyhJiIgoSaSgQXUiIkoSKWlQnYiIkkRKWqlORERJIqVOnaBPHyUJEalsShJpaFCdiFQ6JYk0qqth8WL44IO4IxERiYeSRBoaVCcilU5JIo0tt4R27ZQkRKRyKUmk0aKFBtWJSGVTkmhCYlDdF1/EHYmISOHFtXzpGWb2rplNNrN7zWwtM9vIzF4zsxlmdr+ZtY4jtoaqq+G77zSoTkQqU8GThJl1BU4F+rn7lkAL4EjgCuA6d+8NLAF+UejYGqNBdSJSyeKqbmoJtDGzlsDawAJgd2BM9Pwo4OCYYltJx46wySZKEiJSmQqeJNx9PnA1MIeQHJYBE4Gl7r4ietk8oGtj7zezYWZWa2a1dXV1hQiZAQM0qE5EKlMc1U0dgMHARsAGQFtgn0zf7+4j3b2fu/erqqrKU5Qrq66GujqYObMgpxMRKRpxVDcNAj5w9zp3/xZ4CNgJWDeqfgLYEJgfQ2yN0qA6EalUcSSJOcAAM1vbzAzYA3gPqAEOi14zFBgbQ2yN0qA6EalUcbRJvEZooH4DeCeKYSRwLnCmmc0AOgK3FTq2VK65ZtUZYWtq4Mor44tJRKQQWjb9ktxz94uAixrsngnsEEM4TerfH/7wB/jqqzCo7vXXYcgQGD067shERPJLI64zMHAgXHABfP89nHBCfYIYODDuyERE8ktJIkNnnhnaJe67D046SQlCRCqDkkSGXnmlfpzEX/4S2iRERMqdkkQGamrqq5g6doS+fcO2EoWIlDsliQxMmBASxH77wWmnhV5Ol10W9ouIlDPzEp5rol+/fl5b4OlZlyyBHj1g//3h3nsLemoRkZwws4nu3i+T16ok0UwdOsCvfx1KFtOnxx2NiEh+KUlk4YwzoHVruOKKuCMREckvJYksdO4cxkvceSfMnRt3NCIi+aMkkaWzzw5dYq++Ou5IRETyR0kiS927w89+BrfeCosXxx2NiEh+KEmshnPPDfM5XX993JGIiOSHksRq2HRTOPxwuOkmWLo07mhERHJPSWI1nX8+fPppSBQiIuVGSWI1bbNNGFh33XVhGnERkXISxxrXm5rZpKTbp2Z2upmtZ2ZPm9n06L5DoWPL1vDh8PHHMHJk3JGIiORWHCvTTXX3bdx9G2B74EvgYeA84Fl37wM8G22XhB//GHbbLXSH/frruKMREcmduKub9gDed/fZwGBgVLR/FHBwbFFlYfhw+PBDGDWq6deKiJSKjJOEmbUwsw3MrHviloPzHwkkpsnr7O4LoscLgc4p4hhmZrVmVltXV5eDEHJj0KCwzOkVV8CKFXFHIyKSGxklCTP7DbAIeBp4Iro9vjonNrPWwEHAAw2f8zA1baPT07r7SHfv5+79qqqqVieEnDILpYmZM+H+++OORkQkNzItSZwGbOruW7j7VtHtR6t57n2BN9x9UbS9yMy6AET3JTeO+aCDYIstwloT338fdzQiIqsv0yQxF1iW43MfRX1VE8CjwNDo8VBgbI7Pl3drrBHGTbz7Ljz6aNzRiIisvowWHTKz24BNCdVM/+u/4+7XZnVSs7bAHKCXuy+L9nUERgPdgdnAEHf/JN1x4lh0qCkrVoSR2B07wmuvhWooEZFiko9Fh+YQ2iNaA+2Tbllx9y/cvWMiQUT7Pnb3Pdy9j7sPaipBFKuWLcOcThMmwDPPxB2NiMjqadbypWbWDsDdP89bRM1QjCUJCGMlevWCTTaBmpq4oxERWVnOSxJmtqWZvQm8C7xrZhPNbIvVCbKcrbkmbLcdPP88vPJK/f6aGrjyytjCEhFptkyrm0YCZ7p7D3fvAZwF3Jq/sErfSSeF9ogzzwzbNTUwZEgYSyEiUioyTRJt3f1/FSfu/jzQNi8RlYn99oNjjw2N18OGhQQxejQMHBh3ZCIimcs0Scw0swvNrGd0uwCYmc/AysG110Lr1mH1upNOUoIQkdKTaZI4HqgCHopuVdE+SePNN6FVq/D4hhvUiC0ipSejJOHuS9z9VHffLrqd5u5L8h1cKUu0QTz0EPTuDeusE7aVKESklKRNEmZ2fXT/mJk92vBWmBBL04QJoQ1ir73gmmtg7lw48siwX0SkVLRs4vm7ovur8x1IuTnnnPrHBx4Ie+wBd98N06fHF5OISHOlLUm4+8To4TbuPi75BmyT//DKg1lY3nTZMrj44rijERHJXKYN10Mb2XdsDuMoe1ttBb/6FdxyS5gAUESkFDTVJnGUmT0G9GrQHlEDlOTcSnEaMQLatQsD7JoxG4qISGyaapN4BVgAdAKuSdr/GfB2voIqV506wUUXhSTxr3/B/vvHHZGISHpNTvBnZi2AZ9y96IaCFesEf+l8802oegJ4550w2E5EpJByOsGfu38HfG9m66x2ZELr1mEk9rRpcPPNcUcjIpJeU9VNCZ8D75jZ08AXiZ3ufmpeoipz++0He+8Nf/gDHHNMqIYSESlGmfZuegi4EHgBmJh0y4qZrWtmY8zsv2Y2xcyqzWw9M3vazKZH9x2yPX6xMwulic8+g9//Pu5oRERSy3RajlGE9agTyeGeaF+2bgCedPfNgK2BKcB5wLPu3gd4NtouW5tvHib9+9vfYPLkuKMREWlcposO7QZMB24Cbgammdku2ZwwatvYBbgNwN2/cfelwGAgkXhGAQdnc/xScvHFYU6nM85Ql1gRKU6ZVjddA+zl7ru6+y7A3sB1WZ5zI6AO+IeZvWlmfzeztkBnd18QvWYh0LmxN5vZMDOrNbPaurq6LEMoDh07hnaJZ56Bxx6LOxoRkVVlmiRaufvUxIa7TwNaZXnOlsB2wC3uvi2hIXylqiUP/XIb/dva3Ue6ez9371dVVZVlCMXjxBOhb18466zQPVZEpJhkmiRqo7/4d4tutwLZDlCYB8xz99ei7TGEpLHIzLoARPeLszx+SWnVKjRiz5gBf/lL3NGIiKws0yRxEvAecGp0ew84MZsTuvtCYK6ZbRrt2iM63qPUzxE1FBibzfFL0T77hG6xI0bA4opIjSJSKjJNEie6+7Xufmh0u46QOLL1G+BuM3ubMJvspcDlwJ5mNh0YFG1XjGuugS+/VJdYESkuscwC6+6TonaFH7n7wdHKdx+7+x7u3sfdB7l7RU0guNlmsOOOMHIkvJ00K1ZNDVx5ZXxxiUhly3QW2I0azAL7PJoFNucSCxUNHRq6xCaWQO3fP964RKRyaRbYInLQQXDqqXDDDXDEESFJjB4NA4tuakURqRRNrUw3292fJ7QRvBitSLcA2BCw/IdXea6+Gqqq4IEHQqJQghCROGXaJvECsJaZdQWeAn4G3JGvoCrZiy/CihXQpk1Yxe7BB+OOSEQqWaZJwtz9S+BQ4GZ3PxzYIn9hVaZEG8SDD8Lzz4cxFEccAY8/HndkIlKpMk4SZlYNHA08Ee1rkZ+QKteECfVtEDvsAI88EhqwTz8dvv467uhEpBJlmiROB84HHnb3d82sF1CTv7Aq0znnrNwGsc8+MGoUvP8+/Pzn8P338cUmIpUpo0WHogbrcUnbMwkjryXPjjkGFi6Es8+Gzp1DzydTlwERKZC0ScLMrnf306OxEqtMuOfuB+UtMvmf3/4WFiwIczx16QLnnx93RCJSKZoqSdwV3V+d70AkvauugkWLYPhw+OEP4bjj4o5IRCpB2iTh7hOj+3FmVhU9Lu1FHErUGmvA7bdDXR388pdhLMUBB8QdlYiUuyYbrs3sYjP7CJhKWJGuzsw0DV0MWreGMWNg221DV9lXX407IhEpd03N3XQmsBPQ393Xc/cOwI7ATmZ2RiEClJW1bw9PPAFdu4aSxJQpcUckIuWsqZLEz4Cj3P2DxI6oZ9MxwM/zGZiktv768J//hMF2e+8N8+bFHZGIlKumkkQrd/+o4c6oXSLb5UslB3r1gn//OzRm77wzLFlS/5ymFxeRXGkqSaRbdTnrFZnNbJaZvWNmk8ysNtq3npk9bWbTo/sO2R6/Umy7LVx6KcyaBbvsAsuXa3pxEcmtprrAbm1mnzay34C1VvPcAxuUUs4DnnX3y83svGj73NU8R9k76yxYtgz++EfYemv45JMwg6xmjxWRXGhqqvAW7v6DRm7t3T3X1U2DgVHR41HAwTk+ftkaMQL23RemTw9dZbt2jTsiESkXmc7dlGsOPGVmE81sWLSvs7sviB4vBDo39kYzG2ZmtWZWW1enIRsQqpgmTICf/hQ++ihUQz38cNxRiUg5iCtJ/MTdtwP2BU42s12Sn3R3p5FpQKLnRkbrY/erqqoqQKjFLdEGMXo03H033HMPfPMNHHoonHtuWJtCRCRbsSQJd58f3S8GHgZ2ABaZWReA6H5xHLGVmuTpxQGOPDKsP7HjjqGH0157hR5QIiLZKHiSMLO2ZtY+8RjYC5gMPAoMjV42FBhb6NhKUcPpxSGMnRg/Hu64I4zK3m47jc4WkezEUZLoDLxkZm8BrwNPuPuTwOXAnmY2nbCm9uUxxFZWhg4NyWGttWDXXeHGG8MiRiIimcpoPYlcikZsb93I/o+BPQodT7nbZhuorQ2LFv3mNyFpjBwJbdvGHZmIlIK4Gq6lgDp0gLFj4U9/gnvvhY03hjvvXPk1GqUtIo1RkqgQa6wBv/tdmPNp+XI49tgwvgI0SltEUit4dZPEa8894Z13Qq+niy6CJ58Mg/CSe0iJiCSoJFGBuneHt96C7bcPbRTrrBN6QImINKQkUaFeeQVmz4b994f334ettgolChGRZEoSFSh5lPbjj8N114U1KbbbDp57Lu7oRKSYKElUoIajtE8/He66K3SL3WsvuOWWeOMTkeKhJFGBGhulffTRMG0a7LMP/PrXcPLJ8O238cQnIsVDSUL+5wc/COMpzj4bbr45JIxPPok7KhGJk5KErKRFizCo7o474KWXwkSBU6bEHZWIxEVJQho1dGho4P70UxgwIIynEJHKoyQhKf34x6GRe6ONQlfZAw9ctfeTpvMQKW9KEpJW9+6h2mnw4NBddv/94amnwnOazkOk/ClJSJPatYMxY+CCC+Crr0KiOPPM+rEWms5DpHwpSUhG1lgD/vjHsDyqexiAt8MOYZ0KESlfShLSLD/8Yegq26MH/OtfYTqPt9+OOyoRyZfYkoSZtTCzN83s8Wh7IzN7zcxmmNn9ZtY6rtikcYk2iAcfhA8+gPPOC91jt902DND74ou4IxSRXIuzJHEakNwD/wrgOnfvDSwBfhFLVJJS8nQeZnDZZfDQQ2HOp6uugi22CI3bIlI+YkkSZrYhsD/w92jbgN2BMdFLRgEHxxGbpNbYdB4HHxySxwsvwNprh26yhx0G8+fHE6OI5FZcJYnrgXOA76PtjsBSd18Rbc8Dujb2RjMbZma1ZlZbV1eX/0glIzvvDJMmwSWXwBNPQN++8Oc/w3ffxR2ZiKyOgicJMzsAWOzuE7N5v7uPdPd+7t6vqqoqx9HJ6mjdGoYPh8mToboaTjstNHD/7W8rv04D8ERKRxwliZ2Ag8xsFnAfoZrpBmBdM0ssp7ohoAqLErXxxmEaj3vvhS+/hBNPhP/7vzDFhwbgiZSWgicJdz/f3Td0957AkcBz7n40UAMcFr1sKDC20LFJ7pjBkUfCzJlw0EGhgbtjR9h3X/jtb0P1lIgUv2IaJ3EucKaZzSC0UdwWczySA+uuG6YfP+EEWBG1OJ13Hmy4IZxxBrzxRhicJyLFKdYk4e7Pu/sB0eOZ7r6Du/d298Pd/es4Y5PcqamBRx6BCy+E9u3h4ovD5IE33QTbbx+6zl52WVhzG0J7RU3NqsdQO4ZI4RVTSULKUPJ62iNGhPsbb4Tf/AYWLoS//jVUQw0fDj17wm67weLFcPjh9YlC7Rgi8TEv4bJ+v379vLa2Nu4wJI0rrww/7snjK2pqwtiKc86p3zdzZpgX6q67wjKqrVqFdo3Bg8PrNZGgSO6Y2UR375fRa5UkpJi4Q21tSBa33RZ6R7VsCQccAIccEmag7dgx7ihFSltzkoSqm6SomIWSxyGHhBHcxxwTksRLL4XV8jp3ht13DwP15syJO1qR8qckIUUnuR3jrrvCbLMAt9wC554LixbVD9TbfvswhfkZZ2jVPJF8UJKQopM8kSCE+9Gjw2C8Sy6Bd9+FqVPhiitgzTXh97+H66+HPfcMyeWVV0LCUGO3yOpTm4SUvAUL4LHH4O9/DwkGoEULOOWUUMpo3z7e+ESKjdokpKJ06QLDhsHrr9f3mKqqghtugK5dQ3fb//433hhFSpWShJSNmhq4/fYwaG/FijAeY/BgGDkyzEo7aBA8/HD9yG8RaZqShJSFxgbtXXwxHH88zJ0b2jKmTYNDD4VeveDSS8OgPY3uFklPSULKQqrG7gkTYP31w4jumTPDRIObbAK/+x106wZPPx0SR6JnlEZ3i6xMDddSkaZMgZtvhlGj4LORkRm2AAAL3ElEQVTPwliMAQPg7bfDvoO1LqKUMTVcizShb1/4y1/CMqs33xwaul96KXSzPeQQ6N0bjj46DNp7/XX4Omm6SVVRSSVRkpCK1r49bLYZfPttGKi3zjqhp9TWW8Pzz4dBezvuCD/4QVht7/TTQ8njsMNURSWVoWXTLxEpX8kN3gMHwt57128/+CDMmwevvQbjx4f7kSNh+fLw3kGDoE+fUBq57LKQRETKTcHbJMxsLeAFYE1Ckhrj7heZ2UaE5Uw7AhOBn7n7N+mOpTYJWV2ZzlKb8O23YQ3v8ePDNOdvv13/XOvW0K8f7LRTuP34x6EaK5vziORTUc8Ca2YGtHX3z82sFfAScBpwJvCQu99nZn8F3nL3W9IdS0lC4pIogZx0UmjTOO200J7x8sswcSJ8E/1506dPSBgdO4YxHGPGhAkKG5ZgRAqpOUmi4NVNHrLS59Fmq+jmwO7AT6P9o4CLgbRJQiQODX/gBw6s377qKvjqq5AoXn453B57DD7+OLx30CDYeGP48EO44IKwKp9IMYulC6yZtSBUKfUGbgKuAsa7e+/o+W7Av919y0beOwwYBtC9e/ftZyfWvBQpkOZWHbmHgXwvvxx6VE2atPLz3buH4yVu228fGtBVRSX5UtTVTSud3Gxd4GHgQuCOTJJEMlU3SSlpWEV1wQXw3XfhR3/ChDDYL2HTTcNgv1dfhcsvh1/8IrSDqIpKcqFkxkm4+1KgBqgG1jWzRPXXhsD82AITybGG04Y88ECYKmS77eC+++D99+Gjj+DJJ8PMtZtuGqZE/+KLMEFh27ZhKvR+/eC990LCSPSyakjjOCSXCp4kzKwqKkFgZm2APYEphGRxWPSyocDYQscmki/ppg1J6NgxdMG94AIYOza0W8yfH5KLO/TsGdo6TjkldLdt3z6M5zj+eLjppvrE0b9/eE8iUWgch6yOOHo3/YjQMN2CkKRGu/sIM+tF6AK7HvAmcIy7f536SKpukvKXXEV1yy1w//2hx1RtbUgYiVtdXXh9ixahMXyDDcII8gMPDKWTf/4T9tsv9XnU/lFZSqZNYnUpSUg5a9iLKlW3Wfcw021y0qitDdVXyTp0CEu+du9ef0tsz50Lv/510+eS8qAkIVIGVuev++eeg8MPh4MOCiPHjzwSWrWC2bNhzpxwW7Zs5fe0aBESzpZbwqxZIUHsvXfOP5YUASUJkQqWaQlk2bL6hDFnTkggjz4aZsiF0A138OCQbPbcM6wnLuWhZHo3iUjuZdJIDiEJbLUV7L9/aPPYe+/QtjF8eJjQcMCAkDQOPBA6d4ahQ+Hxx+tnxFUvqsqgkoSIpCx93H13WO71gQfgkUdg6dL6EsZmm8G116odoxSpJCEizZKq9DFpUugV9Y9/wKJF8MQTYb2NRx8NJY7ly2HffcMiTYceGnpg7bZb6vOo9FF6VJIQkWb75ht45plQwrj33pUXZWrTJozp2GijsJ74RhvVP547F447TqWPuBX1BH8iUvpatw4ljDZtQjvF8ceHqqljjw29qGbOhA8+qF/tL1n79rDXXmG8x6xZYcqRTz+FN98MXXI7dACz+tdrDEe8VJIQkaxk0ovKHZYsCQkjkTg++ACeeipst2gR5q9K1rbtymM4Em0iI0aEqq4ZM0KXXpU+sqcusCKSd9n+hd9wFPnf/haSQaIbbnKX3Dlz6keTJ+vdG3bYATbfPKxX3rdv2Neq1erHVwmUJESkKGU6hiPZ8uWhLWPEiFClteOOoUrqvfdCEklo2TJUYfXtG5LH99+H2XbvuSc0rmdyrkpJLGqTEJGilG4MR6of7jZtwkSH//kPXHhhKH0kjvH55zB1akgYU6aE2+TJYYLERDXWfvuFdpDly2HXXUOD+4wZoXG9Z89QikkMFExMjthYEqtUKkmISFHLpvTx9dchEbz3Xpghd9y40MZhFkolDdtBunSpTxpmoYvv4MHw73+H9pDdd08dXymWPlSSEJGykU3pY801w2y4ixeHdTmSSyA77xymYZ81a9Xb+PEhiaxYEaq2IIw432STsMZH8m2TTUIJpdxLHypJiEhZyqYEAqE66ogjQjXVI4+E7rrLl4dqrVmzQltHwgYbhGTRrl04/gEHhJ5bd94ZHqcSd+lDJQkRqXjZlEBqauCoo2DMmMYTy1dfhVUEp06tv02bBm+/HVYRvP/+cJwDDwzTl/ToUd+VN/m+V6/sSh9xJBeVJEREIqvTrTcxNfuYMSHRtG5d34139uww71Wyli1DqaRbN1iwIMy0u8UWsN56ofdWY/e1taGUs7oj1ou6C6yZdQPuBDoDDox09xvMbD3gfqAnMAsY4u5L0h1LSUJE4pZptdann9YnjETyePzx0GbSqVMY4/Hxx2HKk1RatAiDDb/4IjSsv/BCdoMKiz1JdAG6uPsbZtYemAgcDBwLfOLul5vZeUAHdz833bGUJEQkbrkaVDh6dJgccfnyMEr9k09WvU88Hjcu9Ny68MIwfqS5mpMkcPdYb8BYYE9gKiF5AHQBpjb13u23395FRErNc8+5d+oU7hvbzuS9F16Y+XsaAmo9w9/oWKcKN7OewLbAa0Bnd18QPbWQUB3V2HuGmVmtmdXWNTZeX0SkyGW6MFRDyVVZI0aE+yFDVp1+PZdia7g2s3bAOOASd3/IzJa6+7pJzy9x9w7pjqHqJhGpJLnq3VT0XWDNrBXwIHC3uz8U7V5kZl3cfUHUbrE4jthERIpVY4lg4MD8zoZb8OomMzPgNmCKu1+b9NSjwNDo8VBCW4WIiMQojpLETsDPgHfMbFK0bzhwOTDazH4BzAaGxBCbiIgkKXiScPeXAEvx9B6FjEVERNKLtXeTiIgUNyUJERFJqaTnbjKzOkL7RSfgo5jDKQa6DoGuQz1di0DXIUhchx7uXpXJG0o6SSSYWW2mfX7Lma5DoOtQT9ci0HUIsrkOqm4SEZGUlCRERCSlckkSI+MOoEjoOgS6DvV0LQJdh6DZ16Es2iRERCQ/yqUkISIieaAkISIiKZV8kjCzfcxsqpnNiFa0q0hmNsvM3jGzSWZWMfOnm9ntZrbYzCYn7VvPzJ42s+nRfdop58tBiutwsZnNj74Tk8xsvzhjLAQz62ZmNWb2npm9a2anRfsr6juR5jo0+ztR0m0SZtYCmEZY2W4eMAE4yt3fizWwGJjZLKCfu1fUgCEz2wX4HLjT3beM9l1JM5fCLXUprsPFwOfufnWcsRVSLpdHLmVprsMQmvmdKPWSxA7ADHef6e7fAPcBg2OOSQrI3V8APmmwezAwKno8ivCfo6yluA4Vx90XuPsb0ePPgClAVyrsO5HmOjRbqSeJrsDcpO15ZHkhyoADT5nZRDMbFncwMctoKdwKcYqZvR1VR5V1FUtD2SyPXI4aXAdo5nei1JOE1PuJu28H7AucHFU/VLxo0ffSrVNdPbcAGwPbAAuAa+INp3Ci5ZEfBE5390+Tn6uk70Qj16HZ34lSTxLzgW5J2xtG+yqOu8+P7hcDDxOq4irVoqhONlE3W5FL4br7Inf/zt2/B26lQr4T6ZZHjp6viO9EY9chm+9EqSeJCUAfM9vIzFoDRxKWQa0oZtY2apzCzNoCewGT07+rrGkpXP73Y5hwCBXwndDyyEGq65DNd6KkezcBRF24rgdaALe7+yUxh1RwZtaLUHqAsNrgPZVyHczsXmA3whTIi4CLgEeA0UB3oqVw3b2sG3VTXIfdCNUKDswCfpVUL1+WzOwnwIvAO8D30e7hhPr4ivlOpLkOR9HM70TJJwkREcmfUq9uEhGRPFKSEBGRlJQkREQkJSUJERFJSUlCRERSUpIQySEz65k8E6tIqVOSEBGRlJQkRPLEzHqZ2Ztm1j/uWESy1TLuAETKkZltSpi6/lh3fyvueESypSQhkntVhLmBDq3EBbCkvKi6SST3lgFzgJ/EHYjI6lJJQiT3viHMsPkfM/vc3e+JOyCRbClJiOSBu39hZgcAT0eJouKmsJfyoFlgRUQkJbVJiIhISkoSIiKSkpKEiIikpCQhIiIpKUmIiEhKShIiIpKSkoSIiKT0/5Xl+hM0o8m6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x229f539b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clustering dataset\n",
    "# determine k using elbow method\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# k means determine k\n",
    "distortions = []\n",
    "K = range(1,25)\n",
    "for k in :\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(X_embedded)\n",
    "    kmeanModel.fit(X_embedded)\n",
    "    silDists = np.min()\n",
    "    distortions.append()\n",
    " \n",
    "\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = list(word_vectors.vocab.keys())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will choose k = 4\n",
    "kmeanModel = KMeans(n_clusters=4).fit(X_embedded)\n",
    "label = kmeanModel.fit_predict(X_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['vocab']= vocab\n",
    "df['label']= label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>on</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>##</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>with</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>said</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vocab  label\n",
       "0  </s>      0\n",
       "1    in      0\n",
       "2   for      0\n",
       "3  that      0\n",
       "4    is      0\n",
       "5    on      0\n",
       "6    ##      2\n",
       "7   The      2\n",
       "8  with      1\n",
       "9  said      1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>on</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>at</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>as</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>from</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>are</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>this</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>about</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>out</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>after</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>first</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>when</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>over</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>last</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>into</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>just</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>some</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>only</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>back</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>now</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>before</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     vocab  label\n",
       "0     </s>      0\n",
       "1       in      0\n",
       "2      for      0\n",
       "3     that      0\n",
       "4       is      0\n",
       "5       on      0\n",
       "11     the      0\n",
       "12      at      0\n",
       "14      as      0\n",
       "17    from      0\n",
       "19     are      0\n",
       "28    this      0\n",
       "45     one      0\n",
       "46   about      0\n",
       "49     out      0\n",
       "52     all      0\n",
       "55   after      0\n",
       "56   first      0\n",
       "61    when      0\n",
       "63    over      0\n",
       "64    last      0\n",
       "66   other      0\n",
       "69    into      0\n",
       "76    just      0\n",
       "78    some      0\n",
       "89    only      0\n",
       "95    back      0\n",
       "97     now      0\n",
       "99  before      0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cluster 0 seems to group the prepositions\n",
    "df[df.label==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>not</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>he</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>will</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>they</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>we</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>more</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>you</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>would</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>can</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>He</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>do</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>than</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>We</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>her</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>people</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>In</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>our</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>she</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>could</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>them</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>what</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>like</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>get</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>did</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     vocab  label\n",
       "13     not      3\n",
       "20       I      3\n",
       "22      he      3\n",
       "23    will      3\n",
       "32    they      3\n",
       "38      we      3\n",
       "39    more      3\n",
       "43     you      3\n",
       "47   would      3\n",
       "50     can      3\n",
       "57      He      3\n",
       "58      do      3\n",
       "60    than      3\n",
       "62      We      3\n",
       "67     her      3\n",
       "68  people      3\n",
       "70      In      3\n",
       "71     our      3\n",
       "73       A      3\n",
       "74     she      3\n",
       "75   could      3\n",
       "82    them      3\n",
       "83    what      3\n",
       "87    like      3\n",
       "91     get      3\n",
       "92     did      3"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cluster 3 seems to group pronouns and verbs \n",
    "df[df.label==3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will choose the gutenberg project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis:\n",
    "- author_2- author_1 + author_1_most_famous_book ="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
